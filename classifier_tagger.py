import nltk
from sklearn.metrics import average_precision_score, precision_recall_fscore_support
from nltk.probability import DictionaryProbDist
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from nltk.classify import ClassifierI

KW_TAG = 'I-K'
OUT_TAG = 'O'


class ScaledSklearnClassifier(ClassifierI):
    """Wrapper for scikit-learn classifiers."""

    def __init__(self, estimator, dtype=float, sparse=True):
        """
        :param estimator: scikit-learn classifier object.

        :param dtype: data type used when building feature array.
            scikit-learn estimators work exclusively on numeric data. The
            default value should be fine for almost all situations.

        :param sparse: Whether to use sparse matrices internally.
            The estimator must support these; not all scikit-learn classifiers
            do (see their respective documentation and look for "sparse
            matrix"). The default value is True, since most NLP problems
            involve sparse feature sets. Setting this to False may take a
            great amount of memory.
        :type sparse: boolean.
        """
        self._clf = estimator
        self._encoder = LabelEncoder()
        self._vectorizer = DictVectorizer(dtype=dtype, sparse=sparse)
        self._scaler = StandardScaler(with_mean=not sparse)

    def __repr__(self):
        return "<SklearnClassifier(%r)>" % self._clf

    def classify_many(self, featuresets):
        """Classify a batch of samples.

        :param featuresets: An iterable over featuresets, each a dict mapping
            strings to either numbers, booleans or strings.
        :return: The predicted class label for each input sample.
        :rtype: list
        """
        X = self._vectorizer.transform(featuresets)
        X = self._scaler.transform(X)

        classes = self._encoder.classes_
        try:
            dec_fn_result = self._clf.decision_function(X)[0] if classes[0] == KW_TAG else -self._clf.decision_function(X)[0]
        except AttributeError as e:
            dec_fn_result = 50.0
        return [classes[i] for i in self._clf.predict(X)][0], dec_fn_result

    def prob_classify_many(self, featuresets):
        """Compute per-class probabilities for a batch of samples.

        :param featuresets: An iterable over featuresets, each a dict mapping
            strings to either numbers, booleans or strings.
        :rtype: list of ``ProbDistI``
        """
        X = self._vectorizer.transform(featuresets)
        X = self._scaler.transform(X)
        y_proba_list = self._clf.predict_proba(X)
        return [self._make_probdist(y_proba) for y_proba in y_proba_list], y_proba_list[0][0] if self._encoder.classes_[0] == KW_TAG else y_proba_list[1][0]

    def labels(self):
        """The class labels used by this classifier.

        :rtype: list
        """
        return list(self._encoder.classes_)

    def train(self, labeled_featuresets):
        """
        Train (fit) the scikit-learn estimator.

        :param labeled_featuresets: A list of ``(featureset, label)``
            where each ``featureset`` is a dict mapping strings to either
            numbers, booleans or strings.
        """

        X, y = list(zip(*labeled_featuresets))
        X = self._vectorizer.fit_transform(X)
        X = self._scaler.fit_transform(X)
        y = self._encoder.fit_transform(y)
        self._clf.fit(X, y)

        return self

    def _make_probdist(self, y_proba):
        classes = self._encoder.classes_
        return DictionaryProbDist(dict((classes[i], p)
                                       for i, p in enumerate(y_proba)))


class ClassifierTagger(nltk.SequentialBackoffTagger, nltk.tag.api.FeaturesetTaggerI):
    """
    A sequential tagger that uses a classifier to choose the tag for
    each token in a sentence.  The featureset input for the classifier
    is generated by a feature detector function::

        feature_detector(tokens, index, history) -> featureset

    Where tokens is the list of unlabeled tokens in the sentence;
    index is the index of the token for which feature detection
    should be performed; and history is list of the tags for all
    tokens before index.

    Construct a new classifier-based sequential tagger.

    :param feature_detector: A function used to generate the
        featureset input for the classifier::
        feature_detector(tokens, index, history) -> featureset

    :param train: A tagged corpus consisting of a list of tagged
        sentences, where each sentence is a list of (word, tag) tuples.

    :param backoff: A backoff tagger, to be used by the new tagger
        if it encounters an unknown context.

    :param classifier_builder: A function used to train a new
        classifier based on the data in *train*.  It should take
        one argument, a list of labeled featuresets (i.e.,
        (featureset, label) tuples).

    :param classifier: The classifier that should be used by the
        tagger.  This is only useful if you want to manually
        construct the classifier; normally, you would use *train*
        instead.

    :param backoff: A backoff tagger, used if this tagger is
        unable to determine a tag for a given token.

    :param cutoff_prob: If specified, then this tagger will fall
        back on its backoff tagger if the probability of the most
        likely tag is less than *cutoff_prob*.
    """
    def __init__(self, feature_detector=None, train=None,
                 classifier_builder=nltk.NaiveBayesClassifier.train,
                 classifier=None, backoff=None,
                 cutoff_prob=None, verbose=False, restricted_pos_set=None, ablate=None):
        self._check_params(train, classifier)

        self.ablate = ablate

        nltk.SequentialBackoffTagger.__init__(self, backoff)

        if (train and classifier) or (not train and not classifier):
            raise ValueError('Must specify either training data or '
                             'trained classifier.')

        if feature_detector is not None:
            self._feature_detector = feature_detector
            # The feature detector function, used to generate a featureset
            # or each token: feature_detector(tokens, index, history) -> featureset

        self._cutoff_prob = cutoff_prob
        """Cutoff probability for tagging -- if the probability of the
           most likely tag is less than this, then use backoff."""

        self._classifier = classifier
        """The classifier used to choose a tag for each token."""

        if restricted_pos_set:
            self.restricted_pos_set = restricted_pos_set

        if train:
            self._train(train, classifier_builder, verbose)

    def choose_tag_(self, tokens, pos, timings, talk_positions, index, history):
        # Use our feature detector to get the featureset.
        featureset = self.feature_detector(tokens, pos, timings, talk_positions, index, history, self.ablate)

        # Use the classifier to pick a tag.  If a cutoff probability
        # was specified, then check that the tag's probability is
        # higher than that cutoff first; otherwise, return None.
        if self._cutoff_prob is None:
            return self._classifier.classify(featureset)

        pdist = self._classifier.prob_classify(featureset)
        tag = pdist.max()
        return tag if pdist.prob(tag) >= self._cutoff_prob else None

    def choose_tag(self, tokens, index, history):
        # Use our feature detector to get the featureset.
        featureset = self.feature_detector(tokens, index, history)

        # Use the classifier to pick a tag.  If a cutoff probability
        # was specified, then check that the tag's probability is
        # higher than that cutoff first; otherwise, return None.
        if self._cutoff_prob is None:
            return self._classifier.classify(featureset)

        pdist = self._classifier.prob_classify(featureset)
        tag = pdist.max()
        return tag if pdist.prob(tag) >= self._cutoff_prob else None

    def tag_sents(self, sentences, threshold=False):
        pred_tags = []
        true_tags = []
        y_scores = []
        untagged_sentences = []
        for sentence in sentences:
            history = []
            untagged_sentence, pos, timing, talk_positions, tags = zip(*sentence)
            untagged_sentences.append(untagged_sentence)
            for index in range(len(sentence)):
                if self.restricted_pos_set is None or pos[index] in self.restricted_pos_set:
                    featureset = self.feature_detector(untagged_sentence, pos, timing, talk_positions, index, history, self.ablate)
                    pred_tag, y_score = self._classifier.classify_many([featureset])
                    _, y_score = self._classifier.prob_classify_many([featureset])
                    y_scores.append(y_score)
                    # pred_tags.append(max(probs._prob_dict, key=lambda k: probs._prob_dict[k]))
                    pred_tags.append(pred_tag)
                    true_tags.append(tags[index])
                history.append(tags[index])

        numerical_true_labels = [1 if t == KW_TAG else 0 for t in true_tags]
        numerical_pred_labels = [1 if t == KW_TAG else 0 for t in pred_tags]
        avg_precision = average_precision_score(numerical_true_labels, y_scores)
        p, r, f, _ = precision_recall_fscore_support(numerical_true_labels, numerical_pred_labels, average='binary')

        return untagged_sentences, numerical_true_labels, numerical_pred_labels, y_scores, p, r, f, avg_precision

    def _train(self, tagged_corpus, classifier_builder, verbose):
        """
        Build a new classifier, based on the given training data
        *tagged_corpus*.
        """

        classifier_corpus = []
        if verbose:
            print('Constructing training corpus for classifier.')

        for sentence in tagged_corpus:
            history = []
            untagged_sentence, pos, timing, talk_positions, tags = zip(*sentence)
            for index in range(len(sentence)):
                if self.restricted_pos_set is None or pos[index] in self.restricted_pos_set:
                    featureset = self.feature_detector(untagged_sentence, pos, timing, talk_positions, index, history, self.ablate)
                    classifier_corpus.append((featureset, tags[index]))
                history.append(tags[index])

        if verbose:
            print('Training classifier (%d instances)' % len(classifier_corpus))

        self._classifier = classifier_builder(classifier_corpus)

    def __repr__(self):
        return '<ClassifierBasedTagger: %r>' % self._classifier

    def feature_detector(self, tokens, pos, timings, talk_positions, index, history, ablate):
        """
        Return the feature detector that this tagger uses to generate
        featuresets for its classifier.  The feature detector is a
        function with the signature::

          feature_detector(tokens, index, history) -> featureset

        See ``classifier()``
        """
        return self._feature_detector(tokens, pos, timings, talk_positions, index, history, ablate=ablate)

    def classifier(self):
        """
        Return the classifier that this tagger uses to choose a tag
        for each word in a sentence.  The input for this classifier is
        generated using this tagger's feature detector.
        See ``feature_detector()``
        """
        return self._classifier
